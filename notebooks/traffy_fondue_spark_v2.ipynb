{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-TO1bftAQs5",
        "outputId": "4ee0a013-4477-427e-b3be-d1323d89caf0"
      },
      "outputs": [],
      "source": [
        "# !gdown 1uNAYBPerV6IaiWXLPF4Efkxb4Gg-U4Cz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fl3Vt5VmgLwq"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AUkD9aFIiLBY",
        "outputId": "d78fc27a-36ef-4368-ce7b-91afbb5caceb"
      },
      "outputs": [],
      "source": [
        "# if IN_COLAB:\n",
        "#   # !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#   # !wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz\n",
        "#   # !tar xf spark-3.1.1-bin-hadoop3.2.tgz\n",
        "#   # !pip install -q findspark\n",
        "#   # import os\n",
        "#   # os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#   # os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.1-bin-hadoop3.2\"\n",
        "#     !apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#     !wget -q https://dlcdn.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz\n",
        "#     !tar xf spark-3.3.2-bin-hadoop3.tgz\n",
        "#     !mv spark-3.3.2-bin-hadoop3 spark\n",
        "#     !pip install -q findspark\n",
        "#     import os\n",
        "#     os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#     os.environ[\"SPARK_HOME\"] = \"/content/spark\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "KX6gMQ6tBG-P"
      },
      "outputs": [],
      "source": [
        "# import findspark\n",
        "# findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XsAyphMiBH5w"
      },
      "outputs": [],
      "source": [
        "spark_url = 'local'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "03_dzgDqiMgv"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\\\n\u001b[0;32m      4\u001b[0m         \u001b[39m.\u001b[39;49mmaster(spark_url)\\\n\u001b[0;32m      5\u001b[0m         \u001b[39m.\u001b[39;49mappName(\u001b[39m'\u001b[39;49m\u001b[39mtime-taken-teller\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[0;32m      6\u001b[0m         \u001b[39m.\u001b[39;49mconfig(\u001b[39m'\u001b[39;49m\u001b[39mspark.ui.port\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m4040\u001b[39;49m\u001b[39m'\u001b[39;49m)\\\n\u001b[1;32m----> 7\u001b[0m         \u001b[39m.\u001b[39;49mgetOrCreate()\n",
            "File \u001b[1;32mc:\\Users\\supak\\anaconda3\\envs\\torch2\\Lib\\site-packages\\pyspark\\sql\\session.py:477\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    475\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    476\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    478\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
            "File \u001b[1;32mc:\\Users\\supak\\anaconda3\\envs\\torch2\\Lib\\site-packages\\pyspark\\context.py:512\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    511\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 512\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    513\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    514\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
            "File \u001b[1;32mc:\\Users\\supak\\anaconda3\\envs\\torch2\\Lib\\site-packages\\pyspark\\context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    193\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m     )\n\u001b[1;32m--> 198\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\supak\\anaconda3\\envs\\torch2\\Lib\\site-packages\\pyspark\\context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    431\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 432\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    433\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
            "File \u001b[1;32mc:\\Users\\supak\\anaconda3\\envs\\torch2\\Lib\\site-packages\\pyspark\\java_gateway.py:103\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39m# Wait for the file to appear, or for the process to exit, whichever happens first.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m proc\u001b[39m.\u001b[39mpoll() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 103\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    105\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m    106\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mJava gateway process exited before sending its port number\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder\\\n",
        "        .master(spark_url)\\\n",
        "        .appName('time-taken-teller')\\\n",
        "        .config('spark.ui.port', '4040')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKqo-RTyiSou"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder\\\n",
        "        .master(spark_url)\\\n",
        "        .appName('time-taken-teller')\\\n",
        "        .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndnzcUjViT1u"
      },
      "outputs": [],
      "source": [
        "path = '../data/bangkok_traffy.csv'\n",
        "df = spark.read.option(\"delimiter\", \",\").option(\"header\", True).csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfgjR65TAJXz",
        "outputId": "595f258c-07a6-4126-bad5-849478bece55"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqfJL2nwDQWF"
      },
      "outputs": [],
      "source": [
        "# columns_to_drop = ['photo', 'photo_after', 'address', 'star']\n",
        "columns_to_drop = ['ticket_id', 'organization','comment','photo','photo_after','coords','address','star','count_reopen']\n",
        "df = df.drop(*columns_to_drop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POdPXRegBhDt",
        "outputId": "a9cc1eec-fd35-401f-e7de-0ead35bcb023"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6du1T_8wBmTu"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import avg, min, max, countDistinct, split, explode\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql.functions import col, udf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P40q7ClLCOVV",
        "outputId": "48e7f30b-e4ce-4c52-bac6-b0e35641af69"
      },
      "outputs": [],
      "source": [
        "df.show(5), df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U68pITnbBvBY"
      },
      "outputs": [],
      "source": [
        "df = df.filter((col('province').rlike('กรุงเทพ|bangkok')) & (col('province').isNotNull()))\n",
        "df = df.drop(*['province'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evRDp6P-B9o7",
        "outputId": "7485caa4-82d8-4a55-dda4-fb485bb1e574"
      },
      "outputs": [],
      "source": [
        "df.show(5), df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85zAxYBRMNcE",
        "outputId": "db4eeb45-e962-4dfe-cbb0-373e9dfc564a"
      },
      "outputs": [],
      "source": [
        "df = df.dropna(subset=['subdistrict', 'district'])\n",
        "df.show(5), df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKZr8Yf0CZXw"
      },
      "outputs": [],
      "source": [
        "# Define a user-defined function (UDF) to convert the column values\n",
        "@udf(ArrayType(StringType()))\n",
        "def split_and_strip(value):\n",
        "    return [e.strip() for e in value.strip('{}').split(',') if e]\n",
        "\n",
        "# Convert 'type' column to string type\n",
        "df = df.withColumn('type', col('type').cast(StringType()))\n",
        "\n",
        "# Apply the UDF to split and strip the values in 'type' column\n",
        "df = df.withColumn('type', split_and_strip(col('type')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc0XwbGnCs6v",
        "outputId": "cbabf3a4-6dc7-4eeb-814c-5a86e2871826"
      },
      "outputs": [],
      "source": [
        "df.show(5), df.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwRQQWFGEeeV"
      },
      "outputs": [],
      "source": [
        "df = df.filter(col('state').rlike('เสร็จ'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2dztNvdE-oj"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, datediff\n",
        "\n",
        "# Convert 'timestamp' and 'last_activity' columns to datetime\n",
        "df = df.withColumn('timestamp', col('timestamp').cast('timestamp'))\n",
        "df = df.withColumn('last_activity', col('last_activity').cast('timestamp'))\n",
        "\n",
        "# Calculate the difference in days\n",
        "df = df.withColumn('time', datediff(col('last_activity'), col('timestamp')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lq0bNwJJFBdF",
        "outputId": "d07c2273-eafb-4147-c9a4-7ec87439ec9a"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3kSRIdwGQMY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, month, year, quarter, dayofweek\n",
        "\n",
        "# Extract the month\n",
        "df = df.withColumn('month', month(col('timestamp')))\n",
        "\n",
        "# Extract the year\n",
        "df = df.withColumn('year', year(col('timestamp')))\n",
        "\n",
        "# Extract the quarter of the year\n",
        "df = df.withColumn('quarter', quarter(col('timestamp')))\n",
        "\n",
        "# Determine if the day is a weekend\n",
        "df = df.withColumn('is_weekend', (dayofweek(col('timestamp')) >= 6).cast('integer'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGN52yxjIQ9O",
        "outputId": "5c6184a7-1d52-4dd6-bebc-c92078b6d5f9"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYCC4GsaFw_w",
        "outputId": "ffd4144c-901b-4c92-c878-a466dd2248f1"
      },
      "outputs": [],
      "source": [
        "df.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkqpYlDPPni2"
      },
      "outputs": [],
      "source": [
        "# preprocess_w_spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THtbqMieO7wx"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "def array_to_string(my_list):\n",
        "    return '[' + ','.join([str(elem) for elem in my_list]) + ']'\n",
        "\n",
        "array_to_string_udf = udf(array_to_string, StringType())\n",
        "\n",
        "df = df.withColumn('type', array_to_string_udf(df[\"type\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tuW_kKQP_JZ",
        "outputId": "5c956eae-8d18-4c3d-9acc-928e192ad97c"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQgfhbeiQ4Cy",
        "outputId": "287f5b47-bdb3-4e76-a991-707cd3e3459e"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq1J_kgAXAz_",
        "outputId": "09e4af5c-7c66-47d5-edc2-18f365d4c841"
      },
      "outputs": [],
      "source": [
        "df.na.drop(subset=['type',\n",
        " 'subdistrict',\n",
        " 'district',\n",
        " 'timestamp',\n",
        " 'state',\n",
        " 'last_activity',\n",
        " 'time',\n",
        " 'month',\n",
        " 'year',\n",
        " 'quarter',\n",
        " 'is_weekend'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "8Scq2gP0Xrro",
        "outputId": "a32fc02a-b55f-4166-b782-366a3402d474"
      },
      "outputs": [],
      "source": [
        "!export PYSPARK_PYTHON=/usr/bin/python3\n",
        "export SPARK_DRIVER_PYTHON=/usr/bin/python3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "AuC8x6VMXEth",
        "outputId": "af832b29-48ff-4a40-9f4e-0bcea9439a33"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXLnDnIXSMdN"
      },
      "outputs": [],
      "source": [
        "# df = df.dropna(subset=['subdistrict', 'district'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ-k5fQtRpP5"
      },
      "outputs": [],
      "source": [
        "# df.write.option(\"header\",True).csv(\"/tmp/spark_output/datacsv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rldx3CN1W2PB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "PWvqRVFuVkrE",
        "outputId": "bc9fb689-1da5-45be-be6c-a8d5fe501192"
      },
      "outputs": [],
      "source": [
        "df.coalesce(1).write.option(\"header\",True).csv(\"dffile.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "A2eFM19QVUWg",
        "outputId": "34e11cf8-1bd1-41b3-f9e5-c5ff3510f892"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
